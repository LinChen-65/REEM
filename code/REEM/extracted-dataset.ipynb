{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Extracted RE`EM model training dateset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "def391736aa405c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import radians, sin, cos, arcsin, sqrt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a2927e367f73a18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "city = 'Tucson'\n",
    "data_path = './'\n",
    "save_path = './' #ATTENTION"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "404ff7a29986767e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### POI Population "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b5ff3691b54ead6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def disN7(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    d_lon = lon2 - lon1\n",
    "    d_lat = lat2 - lat1\n",
    "    aa = sin(d_lat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(d_lon / 2) ** 2\n",
    "    bb=sqrt(aa)\n",
    "    c = 2 * arcsin(bb)\n",
    "    r = 6371\n",
    "    return c * r"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae015344ee60fc40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cbg_feature_df = pd.read_csv(data_path+f'/data/{city}/{city}_cbg_features_group.csv',usecols=['census_block_group', 'Total population','longitude', 'latitude','average_income_group', 'Hispanic_ratio', 'non_Hispanic_white_ratio','non_Hispanic_black_ratio', 'asian_ratio'])\n",
    "cbg_feature_df.rename(columns={'non_Hispanic_white_ratio':'white_ratio','non_Hispanic_black_ratio':'black_ratio'},inplace=True)\n",
    "racial_feature_list=['Hispanic', 'black', 'asian','white']\n",
    "cbg_feature_df['other_ratio']=1-(cbg_feature_df[[race+'_ratio'for race in racial_feature_list]].sum(axis=1))\n",
    "racial_feature_list.append('other')\n",
    "cbg_feature_df[[item+'_ratio' for item in racial_feature_list]] = cbg_feature_df[[item+'_ratio' for item in racial_feature_list]].round(6) \n",
    "# cbg_feature_df.head() #1328*10"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e2cae55aa8972ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "image_files = os.listdir(data_path+f'/data/{city}/')\n",
    "pattern = f'{city}_poi_with_yelp_review_image_imagestext_'\n",
    "csv_files = [file for file in image_files if file.startswith(pattern) and file.endswith('.csv')]\n",
    "poi_df = pd.read_csv(data_path+f'/data/{city}/'+csv_files[0],usecols=['placekey','longitude','poi_cbg','latitude']) # usecols=['placekey', 'images_text','attributes'])\n",
    "# poi_df = pd.merge(seg_df,review_df,on='placekey',how='inner')\n",
    "\n",
    "def getracial(cbg_row):\n",
    "    racial_array = np.array([])\n",
    "    Total = cbg_row['Total population'].sum()\n",
    "    for racial in racial_feature_list:\n",
    "        proportion=np.sum(cbg_row[racial+'_ratio']*cbg_row['Total population'])/Total\n",
    "        racial_array = np.append(racial_array, round(proportion,6))\n",
    "    return racial_array    \n",
    "\n",
    "poi_df['self'] = poi_df['poi_cbg'].apply(lambda x:getracial(cbg_feature_df[cbg_feature_df['census_block_group']==x]))\n",
    "poi_df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3543b7b7ed9a15a9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7e034289b13728",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_array = np.array([])\n",
    "for racial in racial_feature_list:\n",
    "    proportion=np.sum(cbg_feature_df[racial+'_ratio']*cbg_feature_df['Total population'])/cbg_feature_df['Total population'].sum()\n",
    "    mean_array = np.append(mean_array, round(proportion,6))\n",
    "poi_df['self'] = poi_df['self'].apply(lambda x: mean_array if np.all(np.isnan(x)) else x)\n",
    "\n",
    "poi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "distance = [0.5,1,2,5,10]\n",
    "\n",
    "def getsurround(lon1,lat1,dis):\n",
    "    distance = disN7(lon1,lat1,cbg_feature_df['longitude'],cbg_feature_df['latitude'])\n",
    "    cbg_row = cbg_feature_df[distance<dis]\n",
    "    if cbg_row.empty:\n",
    "        return np.NaN\n",
    "    racial_array = getracial(cbg_row)\n",
    "    return racial_array\n",
    "\n",
    "for dis in distance:\n",
    "    poi_df[dis] = poi_df.apply(lambda x:getsurround(x['longitude'],x['latitude'],dis),axis=1)\n",
    "    print(dis,poi_df[dis].isna().sum())\n",
    "    poi_df[dis] = poi_df[dis].fillna(poi_df['self'])\n",
    "\n",
    "poi_df.to_csv(save_path+f'/data/train-dataset/{city}/{city}_gravity_datasets.csv',index=False)\n",
    "poi_df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cb5349bad9e3b51"
  },
  {
   "cell_type": "markdown",
   "id": "d8e0609b235a8d74",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Embedding Component Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c105e17d17fcb627",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "\n",
    "max_review_num = 50\n",
    "MAX_TRAIN_REVIEW=15  \n",
    "image_files = os.listdir(data_path+f'/data/{city}/')\n",
    "pattern = f'{city}_poi_with_yelp_review_image_imagestext_'\n",
    "csv_files = [file for file in image_files if file.startswith(pattern) and file.endswith('.csv')]\n",
    "review_df = pd.read_csv(data_path+f'/data/{city}/'+csv_files[0],usecols=['placekey','review_num','review'])\n",
    "\n",
    "seg_df = pd.read_csv(data_path+f'/data/{city}/{city}_2019_segregationindex.csv',usecols=['placekey','racial_segregation_index'])\n",
    "df = pd.merge(review_df,seg_df,on='placekey',how='inner')\n",
    "train_df, temp_test_df, _, _ = train_test_split(df, df, test_size=0.4, random_state=42)\n",
    "train_df['review'] = train_df['review'].apply(lambda x:ast.literal_eval(x)) \n",
    "temp_test_df['review'] = temp_test_df['review'].apply(lambda x:ast.literal_eval(x)) \n",
    "\n",
    "val_df, test_df = train_test_split(temp_test_df, test_size=0.5, random_state=42)\n",
    "test_df['review'] = test_df['review'].apply(lambda x:random.sample(x, min(len(x), max_review_num))) \n",
    "test_df['review'] = test_df['review'].apply(lambda x: [f'Review {index+1}: {item[\"text\"]}' for index, item in enumerate(x)])\n",
    "\n",
    "val_df['review'] = val_df['review'].apply(lambda x:random.sample(x, min(len(x), MAX_TRAIN_REVIEW))) \n",
    "val_df['review'] = val_df['review'].apply(lambda x: '\\n'.join([f'Review {index+1}: {item[\"text\"]}' for index, item in enumerate(x)]))\n",
    "\n",
    "\n",
    "def get_sample_count(length):\n",
    "    if length <= 10:\n",
    "        return 1\n",
    "    elif length <= 20:\n",
    "        return 2\n",
    "    elif length <= 40:\n",
    "        return 4\n",
    "    elif length <= 50:\n",
    "        return 5\n",
    "    elif length <= 80:\n",
    "        return 8\n",
    "    else:\n",
    "        return 10\n",
    "\n",
    "def sample_review(review):\n",
    "    sample_count = get_sample_count(len(review))\n",
    "    sampled_reviews = [random.sample(review, min(len(review), MAX_TRAIN_REVIEW)) for _ in range(sample_count)]\n",
    "    return sampled_reviews\n",
    "\n",
    "new_rows = []\n",
    "for index, row in tqdm.tqdm(train_df.iterrows()):\n",
    "    original_review = row['review']\n",
    "   \n",
    "    sampled_reviews = sample_review(original_review)\n",
    " \n",
    "    for review in sampled_reviews:\n",
    "        new_row = row.copy()\n",
    "        new_row['review'] = '\\n'.join([f'Review {index+1}: {item[\"text\"]}' for index, item in enumerate(review)])\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "new_train_df = pd.DataFrame(new_rows).reset_index(drop=True)\n",
    "print(new_train_df.shape)\n",
    "new_train_df.head() \n",
    "\n",
    "\n",
    "population = pd.read_csv(save_path+f'/data/train-dataset/{city}/{city}_gravity_datasets.csv',usecols=['placekey','0.5'])\n",
    "new_train_df = new_train_df.merge(population,on='placekey',how='inner')\n",
    "val_df = val_df.merge(population,on='placekey',how='inner')\n",
    "test_df = test_df.merge(population,on='placekey',how='inner')\n",
    "\n",
    "\n",
    "new_train_df.to_csv(save_path+f'/data/train-dataset/{city}/{city}_popu+allreview_traindata.csv',index=False)\n",
    "val_df.to_csv(save_path+f'/data/train-dataset/{city}/{city}_popu+allreview_valdata.csv',index=False)\n",
    "test_df.to_csv(save_path+f'/data/train-dataset/{city}/{city}_popu+allreview_testdata.csv',index=False)\n",
    "\n",
    "new_train_df.shape,val_df.shape,test_df.shape,new_train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e4eb3a946bc6c7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train/val every review\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "\n",
    "max_review_num = 50\n",
    "MAX_TRAIN_REVIEW=15\n",
    "\n",
    "image_files = os.listdir(data_path+f'/data/{city}/')\n",
    "pattern = f'{city}_poi_with_yelp_review_image_imagestext_'\n",
    "csv_files = [file for file in image_files if file.startswith(pattern) and file.endswith('.csv')]\n",
    "review_df = pd.read_csv(data_path+f'/data/{city}/'+csv_files[0],usecols=['placekey','review_num','review'])\n",
    "seg_df = pd.read_csv(data_path+f'/data/{city}/{city}_2019_segregationindex.csv',usecols=['placekey','racial_segregation_index']) \n",
    "df = pd.merge(review_df,seg_df,on='placekey',how='inner')\n",
    "df['review'] = df['review'].apply(lambda x:ast.literal_eval(x))\n",
    "df['review'] =df['review'].apply(lambda x:random.sample(x, min(len(x), max_review_num))) \n",
    "df['review'] = df['review'].apply(lambda x: [f'Review {index+1}: {item[\"text\"]}' for index, item in enumerate(x)])\n",
    "\n",
    "\n",
    "new_train_df, temp_test_df, _, _ = train_test_split(df, df, test_size=0.4, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_test_df, test_size=0.5, random_state=42)\n",
    "\n",
    "population = pd.read_csv(save_path+f'/data/train-dataset/{city}/{city}_gravity_datasets.csv',usecols=['placekey','0.5'])\n",
    "new_train_df = new_train_df.merge(population,on='placekey',how='inner')\n",
    "val_df = val_df.merge(population,on='placekey',how='inner')\n",
    "test_df = test_df.merge(population,on='placekey',how='inner')\n",
    "\n",
    "\n",
    "new_train_df.to_csv(save_path+f'/data/train-dataset/{city}/{city}_popu+allreview_load_traindata.csv',index=False)\n",
    "val_df.to_csv(save_path+f'/data/train-dataset/{city}/{city}_popu+allreview_load_valdata.csv',index=False)\n",
    "test_df.to_csv(save_path+f'/data/train-dataset/{city}/{city}_popu+allreview_load_testdata.csv',index=False)\n",
    "\n",
    "new_train_df.shape,val_df.shape,test_df.shape #((1072, 5), (1072, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6f5dc0ceb1be5f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## GET Review Embedding\n",
    "Note: The first step of Fusion should be conducted after the Embedding fine-tuned step, hence the following code should be executed after the Embedding Component Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68c048e9781037c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "city = 'Tucson'\n",
    "# file_dir = '/code/RE`EM/model/R&E_embeddingMLP_example/embedding_result' #ATTENTION\n",
    "emb_adapter_foler = 'trained model folder name'\n",
    "file_dir = f\"./code/REEM/trained-gte-model-trained-MLP/{city}/\"+emb_adapter_foler\n",
    "\n",
    "dfs = []\n",
    "for root, dirs, files in os.walk(file_dir):\n",
    "    for filename in files:\n",
    "        if filename.startswith(\"embedding_train_result_\") and filename.endswith(\".csv\"):\n",
    "            file = os.path.join(root, filename)\n",
    "            df = pd.read_csv(file)\n",
    "            dfs.append(df)\n",
    "train_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "dfs = []\n",
    "for root, dirs, files in os.walk(file_dir):\n",
    "    for filename in files:\n",
    "        if filename.startswith(\"embedding_val_result_\") and filename.endswith(\".csv\"):\n",
    "            file = os.path.join(root, filename)\n",
    "            df = pd.read_csv(file)\n",
    "            dfs.append(df)\n",
    "val_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "dfs = []\n",
    "for root, dirs, files in os.walk(file_dir):\n",
    "    for filename in files:\n",
    "        if filename.startswith(\"embedding_test_result_\") and filename.endswith(\".csv\"):\n",
    "            file = os.path.join(root, filename)\n",
    "            df = pd.read_csv(file)\n",
    "            dfs.append(df)\n",
    "test_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "population = pd.read_csv(save_path+f'/data/train-dataset/{city}/{city}_gravity_datasets.csv',usecols=['placekey','0.5'])\n",
    "train_df = train_df.merge(population,on='placekey',how='inner')\n",
    "val_df = val_df.merge(population,on='placekey',how='inner')\n",
    "test_df = test_df.merge(population,on='placekey',how='inner')\n",
    "\n",
    "train_df.to_csv(save_path+f'/data/train-dataset/{city}/{city}_allembedding&popu_traindata.csv',index=False)\n",
    "val_df.to_csv(save_path+f'/data/train-dataset/{city}/{city}_allembedding&popu_valdata.csv',index=False)\n",
    "test_df.to_csv(save_path+f'/data/train-dataset/{city}/{city}_allembedding&popu_testdata.csv',index=False)\n",
    "\n",
    "train_df.shape,val_df.shape,test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1573b3a3e44993a7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Reasoning Component Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee03702fa866ac0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "\n",
    "city = 'target-city'\n",
    "file_dir = f'./data/{city}/rating/'\n",
    "save_path = './'\n",
    "\n",
    "dfs = []\n",
    "for root, dirs, files in os.walk(file_dir):\n",
    "    for filename in files:\n",
    "        if filename.startswith(\"rating_result_\") and filename.endswith(\".csv\"):\n",
    "            file = os.path.join(root, filename)\n",
    "            df = pd.read_csv(file)\n",
    "            dfs.append(df)\n",
    "print(len(dfs))\n",
    "file = pd.concat(dfs, ignore_index=True)\n",
    "file = file.drop(columns=['visitor_home_cbgs_y'])\n",
    "file = file.rename(columns={'visitor_home_cbgs_x': 'visitor_home_cbgs'})\n",
    "print(file.shape)\n",
    "file['rating'] = file['rating'].apply(ast.literal_eval)\n",
    "\n",
    "\n",
    "train_df=pd.read_csv(save_path+f'/data/train-dataset/{city}/{city}_allembedding&popu_traindata.csv')\n",
    "val_df=pd.read_csv(save_path+f'/data/train-dataset/{city}/{city}_allembedding&popu_valdata.csv')\n",
    "test_df=pd.read_csv(save_path+f'/data/train-dataset/{city}/{city}_allembedding&popu_testdata.csv')\n",
    "\n",
    "train_df = train_df.merge(file,on='placekey',how='inner')\n",
    "val_df = val_df.merge(file,on='placekey',how='inner')\n",
    "test_df = test_df.merge(file,on='placekey',how='inner')\n",
    "\n",
    "train_df.to_csv(save_path+f'/data/train-dataset/{city}/{city}_allembedding&popu&rating_traindata.csv',index=False)\n",
    "val_df.to_csv(save_path+f'/data/train-dataset/{city}/{city}_allembedding&popu&rating_valdata.csv',index=False)\n",
    "test_df.to_csv(save_path+f'/data/train-dataset/{city}/{city}_allembedding&popu&rating_testdata.csv',index=False)\n",
    "\n",
    "train_df.shape,val_df.shape,test_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
